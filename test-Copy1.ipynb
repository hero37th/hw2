{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import models\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "train_root = \"/home/yulin/Documents/DLCV/hw2/hw2_train_val/train15000/\"\n",
    "test_root = \"/home/yulin/Documents/DLCV/hw2/hw2_train_val/val1500/\"\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MYDATASET(Dataset):\n",
    "    def __init__(self, root, transform=None):\n",
    "        \"\"\" Intialize the MNIST dataset \"\"\"\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.filenames = []\n",
    "        self.root = root\n",
    "        self.transform = transform\n",
    "        #     parent = os.path.join(path, i)　　read filenames\n",
    "        path = root+\"images/\"\n",
    "        path_ = root+\"labelTxt_hbb/\"\n",
    "        self.filenames=sorted(glob.glob(os.path.join(path,'*.jpg')))\n",
    "        self.txtnames=sorted(glob.glob(os.path.join(path_,'*.txt')))\n",
    "        self.len = len(self.filenames)\n",
    "                              \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\" Get a sample from the dataset \"\"\"\n",
    "        image_fn = self.filenames[index]\n",
    "        temp = Image.open(image_fn).convert('RGB')\n",
    "        image = temp.resize((448, 448),Image.ANTIALIAS)    \n",
    "        f = open(self.txtnames[index])\n",
    "        line = f.readline()\n",
    "        list = []\n",
    "        while line:\n",
    "            a = line.split( )\n",
    "            b = a[0:]\n",
    "            list.append(b)\n",
    "            line = f.readline()\n",
    "        f.close\n",
    "        label_one=np.zeros((7,7,26))\n",
    "        for s in range(len(list)):\n",
    "            x=0.5*(float(list[s][2])+float(list[s][0]))/512\n",
    "            y=0.5*(float(list[s][5])+float(list[s][1]))/512\n",
    "            w=1*(float(list[s][2])-float(list[s][0]))/512\n",
    "            h=1*(float(list[s][5])-float(list[s][1]))/512\n",
    "            class_=list[s][8]\n",
    "            diff_=list[s][9]\n",
    "            i=math.floor(7*x)\n",
    "            j=math.floor(7*y)\n",
    "            if label_one[i][j][0]==0:\n",
    "                \n",
    "                label_one[i][j][0]=x\n",
    "                label_one[i][j][1]=y\n",
    "                label_one[i][j][2]=w\n",
    "                label_one[i][j][3]=h\n",
    "                label_one[i][j][4]=1\n",
    "                if class_==str('plane'):\n",
    "                    label_one[i][j][10]=1\n",
    "                if class_==str('ship'):\n",
    "                    label_one[i][j][11]=1\n",
    "                if class_==str('storage-tank'):\n",
    "                    label_one[i][j][12]=1\n",
    "                if class_==str('baseball-diamond'):\n",
    "                    label_one[i][j][13]=1\n",
    "                if class_==str('tennis-court'):\n",
    "                    label_one[i][j][14]=1\n",
    "                if class_==str('basketball-court'):\n",
    "                    label_one[i][j][15]=1\n",
    "                if class_==str('ground-track-field'):\n",
    "                    label_one[i][j][16]=1\n",
    "                if class_==str('harbor'):\n",
    "                    label_one[i][j][17]=1\n",
    "                if class_==str('bridge'):\n",
    "                    label_one[i][j][18]=1\n",
    "                if class_==str('small-vehicle'):\n",
    "                    label_one[i][j][19]=1\n",
    "                if class_==str('large-vehicle'):\n",
    "                    label_one[i][j][20]=1\n",
    "                if class_==str('helicopter'):\n",
    "                    label_one[i][j][21]=1\n",
    "                if class_==str('roundabout'):\n",
    "                    label_one[i][j][22]=1\n",
    "                if class_==str('soccer-ball-field'):\n",
    "                    label_one[i][j][23]=1\n",
    "                if class_==str('swimming-pool'):\n",
    "                    label_one[i][j][24]=1\n",
    "                if class_==str('container-crane'):\n",
    "                    label_one[i][j][25]=1\n",
    "            else:\n",
    "                pass\n",
    "            label = torch.from_numpy(label_one).type(torch.float)\n",
    "            \n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Total number of samples in the dataset \"\"\"\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset=MYDATASET(root=train_root, transform=transforms.ToTensor())\n",
    "testset=MYDATASET(root=test_root, transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('# images in trainset:', len(trainset)) # Should print 15000\n",
    "#print('# images in testset:', len(testset)) # Should print 1500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image tensor in each batch: torch.Size([1, 3, 448, 448]) torch.float32\n",
      "Label tensor in each batch: torch.Size([1, 7, 7, 26]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "trainset_loader = DataLoader(trainset, batch_size=1, shuffle=True, num_workers=1)\n",
    "testset_loader = DataLoader(testset, batch_size=1, shuffle=False, num_workers=1)\n",
    "# get some random training images\n",
    "dataiter = iter(trainset_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "print('Image tensor in each batch:', images.shape, images.dtype)\n",
    "print('Label tensor in each batch:', labels.shape, labels.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def judge(inputs):\n",
    "    s=7\n",
    "    index=[]\n",
    "    for i in range(s):\n",
    "        for j in range(s):\n",
    "            if labels[0][i][j][0] == 0:\n",
    "                pass\n",
    "            else:\n",
    "                index.append([i,j])\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Yolov1_vgg16bn(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ssd=model(images)\n",
    "#ssd.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index=judge(labels)\n",
    "def iou(inputs, targets, index):\n",
    "    cross=[]\n",
    "    pr=[]\n",
    "    confidence=[]\n",
    "    for i,j in index:\n",
    "        for e in range(2):\n",
    "            squ_label=targets[0][i][j][0:4]\n",
    "            squ_model=inputs[0][i][j][0+e:4+e]\n",
    "            x_min1=squ_label[0]-0.5*squ_label[2]\n",
    "            x_max1=squ_label[0]+0.5*squ_label[2]\n",
    "            y_min1=squ_label[1]-0.5*squ_label[3]\n",
    "            y_max1=squ_label[1]+0.5*squ_label[3]\n",
    "            x_min2=squ_model[0]-0.5*squ_model[2]\n",
    "            x_max2=squ_model[0]+0.5*squ_model[2]\n",
    "            y_min2=squ_model[1]-0.5*squ_model[3]\n",
    "            y_max2=squ_model[1]+0.5*squ_model[3]\n",
    "\n",
    "            ww=min(x_max1,x_max2)-max(x_min1,x_min2)\n",
    "            hh=min(y_max1,y_max2)-max(x_min1,y_min2)\n",
    "            if ww<=0 or hh<=0:\n",
    "                cross.append([0])\n",
    "            else:\n",
    "                cross.append([ww*hh])\n",
    "    for k in range(len(index)):\n",
    "        z=max(cross[0+2*k:2+2*k])\n",
    "        if cross[0+2*k]>=cross[1+2*k]:\n",
    "            confidence.append(cross[0+2*k]) \n",
    "            pr.append([0])\n",
    "        else:\n",
    "            confidence.append(cross[1+2*k]) \n",
    "            pr.append([1])\n",
    "    confidence=torch.FloatTensor(confidence)\n",
    "  \n",
    "\n",
    "    return confidence,pr\n",
    "#confidence,pr=iou(ssd,labels,index)\n",
    "\n",
    "#print(confidence)\n",
    "#print(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossFunc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LossFunc, self).__init__()   \n",
    "        #number\n",
    "        s=7\n",
    "        ss=s*s\n",
    "        coord=5\n",
    "        noodj=0.5    \n",
    "    \n",
    "    def forward(self, inputs, targets):\n",
    "        s=7\n",
    "        ss=s*s\n",
    "        coord=5\n",
    "        noodj=0.5\n",
    "        index=judge(targets)\n",
    "        delta_sum=0\n",
    "        for i,j in index:\n",
    "            temp=inputs[:, i, j, 1:3]-targets[:, i, j, 1:3]\n",
    "            delta_xy1=torch.sum(torch.pow(temp,2))\n",
    "            temp2=torch.sqrt(inputs[:, i, j, 3:5])-torch.sqrt(labels[:, i, j, 3:5])\n",
    "            delta_wh1=torch.sum(torch.pow(temp2,2))\n",
    "            temp3=inputs[:, i, j, 6:8]-targets[:, i, j, 1:3]\n",
    "            delta_xy2=torch.sum(torch.pow(temp3,2))\n",
    "            temp4=torch.sqrt(inputs[:, i, j, 8:10])-torch.sqrt(labels[:, i, j, 3:5])\n",
    "            delta_wh2=torch.sum(torch.pow(temp4,2))\n",
    "            delta_sum = delta_sum+delta_wh1+delta_xy1+delta_wh2+delta_xy2\n",
    "        first=coord*delta_sum#中心點誤差\n",
    "        \n",
    "        confidence,pr=iou(inputs, targets, index)#獲得置信度與概率\n",
    "        hat_c=torch.FloatTensor(len(pr),2).zero_()\n",
    "        m=0\n",
    "        second=0\n",
    "        for i,j in index:\n",
    "            hat_c[m][0]=inputs[:, i, j, 4:5]\n",
    "            hat_c[m][1]=inputs[:, i, j, 9:10]\n",
    "            m=m+1\n",
    "        key=[i for i,v in enumerate(pr) if v==[1]]\n",
    "        for s in key:\n",
    "            hat_c[s,:]=hat_c[s,[1,0]]\n",
    "            tar_1=torch.sum(torch.pow(hat_c[:,0]-torch.squeeze(confidence),2))\n",
    "            tar_2=torch.sum(torch.pow(hat_c[:,1],2))\n",
    "            \n",
    "            second=second+tar_1+noodj*tar_2\n",
    "        \n",
    "        third=0\n",
    "        for i,j in index:\n",
    "            obj=torch.squeeze(targets[:, i, j, 10:])\n",
    "            a=torch.eq(obj,1)\n",
    "            b=(a==1).nonzero()\n",
    "            for o in range(len(b)):\n",
    "                c=b[o].item()\n",
    "                d=inputs[:, i, j,10+c:11+c]\n",
    "                e=torch.pow(d-torch.FloatTensor([[1]]),2)\n",
    "                third=third+e\n",
    "            \n",
    "        loss=first+second+third\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval=100):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion=LossFunc()\n",
    "    model.train()  # Important: set training mode\n",
    "    \n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_some(model, epoch, log_interval=100):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion=LossFunc()\n",
    "    model.train()  # Important: set training mode\n",
    "    \n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            #data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train(model, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = {'state_dict': model.state_dict(),\n",
    "             'optimizer' : optimizer.state_dict()}\n",
    "    torch.save(state, checkpoint_path)\n",
    "    print('model saved to %s' % checkpoint_path)\n",
    "    \n",
    "def load_checkpoint(checkpoint_path, model, optimizer):\n",
    "    state = torch.load(checkpoint_path)\n",
    "    model.load_state_dict(state['state_dict'])\n",
    "    optimizer.load_state_dict(state['optimizer'])\n",
    "    print('model loaded from %s' % checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save(model, epoch, save_interval, log_interval=100):\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "    criterion=LossFunc()\n",
    "    model.train()\n",
    "    \n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        for batch_idx, (data, target) in enumerate(trainset_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(data), len(trainset_loader.dataset),\n",
    "                    100. * batch_idx / len(trainset_loader), loss.item()))\n",
    "            if iteration % save_interval == 0 and iteration > 0:\n",
    "                save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)\n",
    "            iteration += 1\n",
    "    \n",
    "    # save the final model\n",
    "    save_checkpoint('mnist-%i.pth' % iteration, model, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a brand new model\n",
    "model = Net().to(device)\n",
    "test(model)\n",
    "train_save(model, 5, 500, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
